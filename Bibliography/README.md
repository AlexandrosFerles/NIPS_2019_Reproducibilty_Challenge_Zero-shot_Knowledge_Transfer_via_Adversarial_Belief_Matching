# Bibliography

Some of the most interesting papers cited in 'Zero-shot Knowledge Transfer via Adversarial Belief Matching'

1. [Variational Information Distillation for Knowledge Transfer](https://arxiv.org/abs/1904.05835): Listed as recent SoTA work in Few-Shot learning. 

2. [Moonshine: Distilling with Cheap Convolutions](https://arxiv.org/abs/1711.02613): Knowledge Transfer via designing better students

3. [Fidelity-Weighted Learning](https://arxiv.org/abs/1711.02799): Fidelity Loss

4. [ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness](https://arxiv.org/abs/1811.12231): Motivation that textures are more informative than shapes.

5. [Adversarial Examples Are Not Bugs, They Are Features](https://arxiv.org/abs/1905.02175) : Use Adversarial Examples as Extra Features.

6. [Few-shot learning of neural networks from scratch by pseudo example optimization](https://arxiv.org/abs/1802.03039):  Hand-Crafted Loss for Few-shot distilation.

7. [Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer](https://arxiv.org/abs/1612.03928): Attention based loss that is used in the main paper.

8. [Wide Residual Networks](https://arxiv.org/abs/1605.07146): Wide ResNets used in the main papers in different variants as teacher and Student architectures.  

